# ğŸ¤– TinyLlama Integration Guide

This guide explains how to use TinyLlama with the Graph-RAG application via Docker.

## ğŸš€ Quick Start with Docker

### Prerequisites
- Docker and Docker Compose installed
- At least 4GB RAM available for containers
- Internet connection (for downloading TinyLlama model)

### Running the Application

1. **Start all services:**
   ```bash
   ./run_docker.sh
   ```

2. **Access the applications:**
   - **Streamlit App**: http://localhost:8501
   - **Neo4j Browser**: http://localhost:7474 (neo4j/password)

### ğŸ§  TinyLlama Features

The application now includes TinyLlama (1.1B parameter model) for:

- **Natural Language Generation**: Convert retrieved documents into fluent responses
- **Context Understanding**: Better comprehension of user queries
- **Response Synthesis**: Combine multiple sources into coherent answers

### ğŸ¯ Usage

1. **Initialize Systems**: Click "ğŸš€ Initialize Systems" in the sidebar
2. **Enable TinyLlama**: Check "Enable TinyLlama LLM" option
3. **Ask Questions**: Try queries like:
   - "Who is Superman?"
   - "What are Batman's powers?"
   - "Who are Superman's teammates?"

### ğŸ“Š Comparison Modes

The app provides several comparison views:

- **ğŸ¤– AI Answers**: TinyLlama-generated responses (Traditional vs Graph RAG)
- **ğŸ“Š Side-by-Side**: Raw search results comparison
- **ğŸ“„ Traditional Details**: Document-based retrieval analysis
- **ğŸ•¸ï¸ Graph Details**: Knowledge graph traversal analysis

### âš™ï¸ Configuration

You can configure TinyLlama behavior by:

- **Enabling/Disabling**: Use the sidebar checkbox
- **Model Selection**: Modify `tinyllama_integration.py` for different models
- **Generation Parameters**: Adjust temperature, max_tokens in the code

### ğŸ”§ Troubleshooting

**Model Loading Issues:**
- TinyLlama will automatically download on first use (~2GB)
- If download fails, check internet connection
- The app falls back to simple text responses if LLM fails

**Performance:**
- First model load takes 1-2 minutes
- Subsequent responses are faster
- CPU-only version is used in Docker for compatibility

**Memory Issues:**
- Ensure Docker has at least 4GB RAM allocated
- TinyLlama uses ~2GB when loaded
- Monitor system resources during use

### ğŸ³ Docker Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit     â”‚    â”‚     Neo4j       â”‚
â”‚   + TinyLlama   â”‚â—„â”€â”€â–ºâ”‚  Knowledge      â”‚
â”‚   (Port 8501)   â”‚    â”‚  Graph          â”‚
â”‚                 â”‚    â”‚  (Port 7687)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“ Example Queries

**Traditional RAG Questions:**
- "What are Wonder Woman's powers?"
- "Tell me about The Flash"

**Graph RAG Questions:**
- "Who are Superman's teammates?"
- "What relationships exist between heroes?"
- "Which heroes are in the Justice League?"

### ğŸ”„ Stopping Services

Press `Ctrl+C` in the terminal running the Docker services, or:

```bash
docker compose down
```
