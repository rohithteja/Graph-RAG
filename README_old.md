# ğŸ¦¸â€â™‚ï¸ Graph RAG vs Traditional RAG Demo

Interactive demonstration comparing Traditional RAG and Graph RAG approaches using superhero data, **powered by fast API-based LLMs**.

## ğŸ¯ Purpose

This is a **demonstration project** to showcase the differences between:
- **ğŸ“„ Traditional RAG**: Keyword matching search â†’ AI generation
- **ğŸ•¸ï¸ Graph RAG**: Knowledge graph traversal â†’ AI synthesis

## âœ¨ Fast API-Based LLM Integration

This project uses **API-based LLMs** for lightning-fast responses:
- ğŸ¤– **API-powered answers** (OpenAI, Groq, HuggingFace, Ollama)
- âš¡ **Lightning fast**: Responses in 1-3 seconds vs 10-30 seconds locally
- ğŸ§  **High-quality responses** from state-of-the-art models
- ğŸ¯ **Context-aware** based on retrieved information
- ğŸ”„ **Mock mode**: Perfect for testing without API keys
- ğŸ³ **Ultra-lightweight Docker**: No model downloads neededaditional RAG Demo

Simple demonstration comparing Traditional RAG and Graph RAG approaches using superhero data, now **powered by TinyLlama** for natural language generation.

## ğŸ¯ Purpose

This is a **demonstration project** to showcase the differences between:
- **Traditional RAG**: Keyword matching search + TinyLlama generation
- **Graph RAG**: Knowledge graph traversal + TinyLlama synthesis

## âœ¨ New: TinyLlama Integration

This project now includes **TinyLlama-1.1B-Chat** for generating natural language responses:
- ğŸ¤– **AI-powered answers** instead of raw document retrieval
- ğŸ§  **Natural language understanding** of queries
- ğŸ¯ **Context-aware responses** based on retrieved information
- ï¿½ **Fallback mechanisms** for reliability

## ï¿½ğŸ› ï¸ Tech Stack

- **Streamlit**: Web interface
- **Neo4j**: Knowledge graph database  
- **API LLMs**: OpenAI, Groq, HuggingFace, or Ollama
- **Requests**: Simple HTTP API calls (no model downloads!)
- **Docker**: Lightweight containerized deployment
- **Python**: Core implementation

## ğŸš€ Quick Start

### Option 1: Docker (Recommended)

1. **Start with Docker**
```bash
./start.sh
```
Or manually:
```bash
docker-compose up --build
```

2. **Access the Application**
- Streamlit App: http://localhost:8501
- Neo4j Browser: http://localhost:7474 (neo4j/password)

### Option 2: Local Development

1. **Install Requirements**
```bash
pip install -r requirements.txt
```

2. **Start Neo4j** (Docker)
```bash
docker run -p 7474:7474 -p 7687:7687 -e NEO4J_AUTH=neo4j/password neo4j:latest
```

3. **Test TinyLlama Integration** (Optional)
```bash
python test_tinyllama.py
```

4. **Run the Demo**
```bash
streamlit run app.py
```

Visit: http://localhost:8501

## ğŸ¤– TinyLlama Features

### AI-Powered Responses
- **ğŸ§  Natural Language Generation**: TinyLlama converts retrieved information into fluent responses
- **ğŸ¯ Context-Aware**: Understands the difference between Traditional and Graph RAG contexts
- **âš¡ Lightweight**: 1.1B parameter model runs on CPU or GPU
- **ï¿½ Fallback Safe**: Automatically falls back to simple responses if model fails

### Configuration Options
- **Enable/Disable TinyLlama**: Toggle AI responses in the sidebar
- **Device Selection**: Automatic GPU/CPU detection
- **Model Status**: Real-time model loading and performance info

## ï¿½ğŸ“Š What You'll See

### Traditional RAG + TinyLlama
- Document-based search with keyword matching
- TinyLlama generates natural responses from retrieved documents
- Good for direct factual questions

### Graph RAG + TinyLlama  
- Relationship-based search through knowledge graph
- TinyLlama synthesizes information from connected entities
- Good for connected questions

## ğŸ¦¸â€â™‚ï¸ Demo Data

Simple superhero knowledge graph:
- Superman, Batman, Wonder Woman, Flash
- Relationships: teammates, allies, enemies
- Teams: Justice League
- Powers and origins